ì´ ì „ ê¸€ì—ì„œëŠ” ë³´ì¡° ì •ë ¬ì— ëŒ€í•´ ë‹¤ë£¨ì—ˆìŠµë‹ˆë‹¤.<br>
ë°ì´í„°ë¥¼ ë„ì¶œí•˜ëŠ”ë° ë³´ì¡°ì ìœ¼ë¡œ ì •ë ¬ì„ ë”í•œë‹¤ëŠ” ê¸°ëŠ¥ì´ì—ˆìŠµë‹ˆë‹¤.<br>
ì´ë²ˆì— ë‹¤ë£° ì •ë ¬ì€ ë¶€ë¶„ ì •ë ¬ ì…ë‹ˆë‹¤.<br>
ë¶€ë¶„ ì •ë ¬ì€ <color4>ê²€ìƒ‰</color4> ì— ìš©ì´í•œ ì •ë ¬ì…ë‹ˆë‹¤.<br>
ì¼ë‹¨ ì‹œì‘í•´ë´…ì‹œë‹¤.<br>
ì´ë²ˆ ê¸€ì˜ íŒŒì¼ë“¤ì€ hadoop-examples í´ë”ì— search í´ë”ë¥¼ ë§Œë“¤ì–´ì„œ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤.<br>
ì¼ë‹¨ sortí´ë” ë‚´ì— <color4>AirlineParser.java</color4> íŒŒì¼ì„ ì—¬ê¸°ë¡œ ë³µì‚¬í•´ì£¼ì„¸ìš”.
<br>
<color4>SequenceFileCreator.java</color4>

<terminal>
package Airline;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.SequenceFile.CompressionType;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.io.compress.GzipCodec;
import org.apache.hadoop.mapred.*;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;

import Airline.AirlineParser;

import java.io.IOException;

public class SequenceFileCreator extends Configured implements Tool {
    static class DistanceMapper extends MapReduceBase implements Mapper&lt;LongWritable, Text, IntWritable, Text&gt; {  // ë§¤í¼ í´ë˜ìŠ¤

        private IntWritable outputKey = new IntWritable();

        public void map(LongWritable key, Text value, OutputCollector&lt;IntWritable, Text&gt; output, Reporter reporter) throws IOException {

            try {
                AirlineParser parser = new AirlineParser(value);
                if (parser.isDistanceAvailable()) {
                    outputKey.set(parser.getDistance());    // í‚¤ê°’ì— ê±°ë¦¬ë¥¼ ë„£ìŒ. ê±°ë¦¬ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ë¨.
                    output.collect(outputKey, value);       // ë§¤í¼ë‚˜ ë¦¬ë“€ì„œ, ë“œë¼ì´ë²„ í´ë˜ìŠ¤ ì—†ì´ ê·¸ëƒ¥ ì‹œí€€ìŠ¤ íŒŒì¼ë¡œ ë³€í™˜ë§Œ í•˜ê¸° ë•Œë¬¸ì— ì»¬ë ‰í„°ê°€ ìˆ˜í–‰í•¨
                }
            } catch (ArrayIndexOutOfBoundsException ae) {
                outputKey.set(0);
                output.collect(outputKey, value);
                ae.printStackTrace();
            } catch (Exception e) {
                outputKey.set(0);
                output.collect(outputKey, value);
                e.printStackTrace();
            }
        }
    }

    public int run(String[] args) throws Exception {    // ë“œë¼ì´ë²„ í´ë˜ìŠ¤
        JobConf conf = new JobConf(SequenceFileCreator.class);
        conf.setJobName("SequenceFileCreator");

        conf.setMapperClass(DistanceMapper.class);  // ìœ„ì— ìˆëŠ” ìŠ¤íƒœí‹± í´ë˜ìŠ¤ê°€ ë§¤í¼ ê¸°ëŠ¥ì„ í•¨
        conf.setNumReduceTasks(0);   // ë¦¬ë“€ìŠ¤ ê¸°ëŠ¥ ìˆ˜í–‰ ì•ˆí•¨

        FileInputFormat.setInputPaths(conf, new Path(args[0]));
        FileOutputFormat.setOutputPath(conf, new Path(args[1]));

        conf.setOutputFormat(SequenceFileOutputFormat.class);   // ì‹œí€€ìŠ¤ íŒŒì¼ë¡œ ì¶œë ¥
        conf.setOutputKeyClass(IntWritable.class);              // í‚¤ê°’ì€ ê±°ë¦¬. intê°’ìœ¼ë¡œë„ í‘œí˜„ ê°€ëŠ¥
        conf.setOutputValueClass(Text.class);                   // ì¶œë ¥ê°’ì€ ë¬¸ì„œ ë¼ì¸ ì „ì²´.... Textë¡œ í•´ì•¼í•¨

        SequenceFileOutputFormat.setCompressOutput(conf, true);                         // ì‹œí€€ìŠ¤ë¡œ ì••ì¶•
        SequenceFileOutputFormat.setOutputCompressorClass(conf, GzipCodec.class);       // Gzip ì½”ë±ì„ ì´ìš©í•˜ì—¬ ì••ì¶•
        SequenceFileOutputFormat.setOutputCompressionType(conf, CompressionType.BLOCK); // ë¸”ë¡ë‹¨ìœ„ë¡œ.

        JobClient.runJob(conf);
        return 0;
    }

    public static void main(String[] args) throws Exception {
        int res = ToolRunner.run(new Configuration(), new SequenceFileCreator(), args);
        System.out.println("###Result : " + res);
    }
}
</terminal>

ë°©ê¸ˆ ì‘ì—…í•œ íŒŒì¼ì„ ìì„¸íˆ ë³´ë©´ í•œ ìë°” íŒŒì¼ ì•ˆì— ë§¤í¼ì™€ ë“œë¼ì´ë²„ í´ë˜ìŠ¤ê°€ ê°™ì´ ìˆìŒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.<br>
íŒŒì¼ì„ ì‹œí€€ìŠ¤ íŒŒì¼ë¡œ ë³€í™˜í•´ì£¼ëŠ” ì½”ë“œì´ë‹ˆ í•œë²ˆ ì‹¤í–‰í•´ë´…ì‹œë‹¤.<br>

<terminal>
<strong>[hadoop@namenode search]$</strong> javac -cp $HADOOP_HOME/hadoop-core-1.2.1.jar -d . *.java
<strong>[hadoop@namenode search]$</strong> jar -cvf $HADOOP_HOME/SequenceFileCreator.jar ./Airline/*.class
Manifestë¥¼ ì¶”ê°€í•¨
ì¶”ê°€í•˜ëŠ” ì¤‘: Airline/AirlineParser.class(ì…ë ¥ = 2223) (ì¶œë ¥ = 1079)(51%ë¥¼ ê°ì†Œí•¨)
ì¶”ê°€í•˜ëŠ” ì¤‘: Airline/SequenceFileCreator$DistanceMapper.class(ì…ë ¥ = 2024) (ì¶œë ¥ = 828)(59%ë¥¼ ê°ì†Œí•¨)
ì¶”ê°€í•˜ëŠ” ì¤‘: Airline/SequenceFileCreator.class(ì…ë ¥ = 2686) (ì¶œë ¥ = 1164)(56%ë¥¼ ê°ì†Œí•¨)
<strong>[hadoop@namenode search]$</strong> cd $HADOOP_HOME
<strong>[hadoop@namenode hadoop]$</strong> ./bin/hadoop jar SequenceFileCreator.jar Airline.SequenceFileCreator airplain/2008.csv 2008_sequencefile
.
.
.
18/01/28 05:06:50 INFO mapred.JobClient:  map 100% reduce 0%
18/01/28 05:06:50 INFO mapred.JobClient: Job complete: job_201801272358_0012
18/01/28 05:06:50 INFO mapred.JobClient: Counters: 20
.
.
.
</terminal>

ì œê°€ ì•„ê¹Œ ì–¸ê¸‰í•˜ëŒ€ë¡œ ë¦¬ë“€ì„œëŠ” ì‘ë™í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.<br>
ë‹¨ì§€ ì‹œí€€ìŠ¤ íŒŒì¼ë¡œ ë³€í™˜í•  ë¿ì´ì§€ ë¦¬ë“€ì„œë¡œ í•©ê³„ë¥¼ ë‚´ê±°ë‚˜ í•˜ì§€ëŠ” ì•Šê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.<br>
ì˜ ëëŠ”ì§€ ì´ì œ í™•ì¸í•´ë´…ì‹œë‹¤.<br>

<terminal>
<strong>[hadoop@namenode hadoop]$</strong> ./bin/hadoop fs -ls 2008_sequencefile
Found 13 items
-rw-r--r--   3 hadoop supergroup          0 2018-01-28 05:06 /user/hadoop/2008_sequencefile/_SUCCESS
drwxr-xr-x   - hadoop supergroup          0 2018-01-28 05:05 /user/hadoop/2008_sequencefile/_logs
-rw-r--r--   3 hadoop supergroup   18795517 2018-01-28 05:05 /user/hadoop/2008_sequencefile/part-00000
-rw-r--r--   3 hadoop supergroup   19043636 2018-01-28 05:05 /user/hadoop/2008_sequencefile/part-00001
-rw-r--r--   3 hadoop supergroup   18346427 2018-01-28 05:05 /user/hadoop/2008_sequencefile/part-00002
-rw-r--r--   3 hadoop supergroup   18560235 2018-01-28 05:05 /user/hadoop/2008_sequencefile/part-00003
-rw-r--r--   3 hadoop supergroup   18389695 2018-01-28 05:06 /user/hadoop/2008_sequencefile/part-00004
-rw-r--r--   3 hadoop supergroup   18268623 2018-01-28 05:06 /user/hadoop/2008_sequencefile/part-00005
-rw-r--r--   3 hadoop supergroup   18389093 2018-01-28 05:06 /user/hadoop/2008_sequencefile/part-00006
-rw-r--r--   3 hadoop supergroup   18544770 2018-01-28 05:06 /user/hadoop/2008_sequencefile/part-00007
-rw-r--r--   3 hadoop supergroup   17387259 2018-01-28 05:06 /user/hadoop/2008_sequencefile/part-00008
-rw-r--r--   3 hadoop supergroup   18226586 2018-01-28 05:06 /user/hadoop/2008_sequencefile/part-00009
-rw-r--r--   3 hadoop supergroup    4855394 2018-01-28 05:06 /user/hadoop/2008_sequencefile/part-00010
</terminal>

part-0 ë¶€í„° 10 ê¹Œì§€ 11ê°œì˜ ì‹œí€€ìŠ¤ íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.<br>
ê¶ê¸ˆí•˜ë‹ˆê¹Œ ë‚´ìš©ì´ë‚˜ í•œë²ˆ ë³¼ê¹Œìš”?<br>

<terminal>
<strong>[hadoop@namenode hadoop]$</strong> ./bin/hadoop fs -cat 2008_sequencefile/part-00000 | head -10
.
.
.
Nï¿½Bpï¿½nï¿½[ï¿½$Eï¿½ï¿½lï¿½ï¿½ï¿½:ï¿½DKX*ï¿½z#sï¿½ï¿½*Nï¿½_ï¿½(ï¿½ ï¿½ï¿½ï¿½ï¿½2ï¿½ï¿½ï¿½13&Kï¿½8?Wo@ï¿½ï¿½ï¿½Nï¿½ï¿½ï¿½Qdï¿½%9ï¿½"uï¿½< @|y^ï¿½ï¿½{xjï¿½sï¿½5Eï¿½qJï¿½Bï¿½%ï¿½~xï¿½$Ú¥ï¿½ï¿½ï¿½Mï¿½ï¿½h.Fï¿½ï¿½ï¿½ï¿½d+ï¿½ï¿½ï¿½%2Cï¿½ï¿½[Ó¡vrAï¿½!ï¿½ISfmï¿½uï¿½,ï¿½HXï¿½ï¿½ï¿½ï¿½lÑ©ï¿½tï¿½ï¿½xï¿½ï¿½ï¿½ï¿½ï¿½Ù°ï¿½ï¿½4|ï¿½ï¿½ï¿½0ï¿½+ï¿½lï¿½ï¿½ï¿½6ï¿½ï¿½ï¿½ï¿½|ï¿½X%0ï¿½pï¿½ï¿½4ï¿½ï¿½y8ï¿½ï¿½ï¿½n]+"ï¿½6ï¿½l_$)ï¿½ï¿½cï¿½ï¿½d;ï¿½ï¿½Hï¿½ï¿½#ï¿½ï¿½ï¿½Yï¿½2ï¿½ï¿½Iï¿½ï¿½NJgc)0ï¿½ï¿½ï¿½ï¿½ï¿½j=,/ï¿½ï¿½&ï¿½ï¿½ï¿½;ï¿½ï¿½K	c')ï¿½ï¿½mï¿½_ï¿½K,Ìï¿½n 2Óƒ}
ï¿½Î¦ï¿½ï¿½ï¿½)Cï¿½ï¿½Æµ,La#ï¿½~ï¿½[%ï¿½qï¿½ï¿½Kï¿½ï¿½&ï¿½ï¿½ï¿½Geï¿½ï¿½%*%ï¿½q<ï¿½ï¿½Ê„ï¿½ï¿½Oï¿½Dï¿½ï¿½
cat: Unable to write to output stream.
</terminal>

ìœ¼ìœ¼... catë¡œ ì½ì„ ìˆ˜ ì—†êµ°ìš” textë¡œ ì½ì–´ë´…ì‹œë‹¤.<br>

<terminal>
<strong>[hadoop@namenode hadoop]$</strong> ./bin/hadoop fs -text 2008_sequencefile/part-00000 | head -10
18/01/28 05:11:16 INFO util.NativeCodeLoader: Loaded the native-hadoop library
18/01/28 05:11:16 INFO zlib.ZlibFactory: Successfully loaded & initialized native-zlib library
18/01/28 05:11:16 INFO compress.CodecPool: Got brand-new decompressor
18/01/28 05:11:16 INFO compress.CodecPool: Got brand-new decompressor
18/01/28 05:11:16 INFO compress.CodecPool: Got brand-new decompressor
18/01/28 05:11:16 INFO compress.CodecPool: Got brand-new decompressor
810	2008,1,3,4,2003,1955,2211,2225,WN,335,N712SW,128,150,116,-14,8,IAD,TPA,810,4,8,0,,0,NA,NA,NA,NA,NA
810	2008,1,3,4,754,735,1002,1000,WN,3231,N772SW,128,145,113,2,19,IAD,TPA,810,5,10,0,,0,NA,NA,NA,NA,NA
515	2008,1,3,4,628,620,804,750,WN,448,N428WN,96,90,76,14,8,IND,BWI,515,3,17,0,,0,NA,NA,NA,NA,NA
515	2008,1,3,4,926,930,1054,1100,WN,1746,N612SW,88,90,78,-6,-4,IND,BWI,515,3,7,0,,0,NA,NA,NA,NA,NA
515	2008,1,3,4,1829,1755,1959,1925,WN,3920,N464WN,90,90,77,34,34,IND,BWI,515,3,10,0,,0,2,0,0,0,32
688	2008,1,3,4,1940,1915,2121,2110,WN,378,N726SW,101,115,87,11,25,IND,JAX,688,4,10,0,,0,NA,NA,NA,NA,NA
1591	2008,1,3,4,1937,1830,2037,1940,WN,509,N763SW,240,250,230,57,67,IND,LAS,1591,3,7,0,,0,10,0,0,0,47
1591	2008,1,3,4,1039,1040,1132,1150,WN,535,N428WN,233,250,219,-18,-1,IND,LAS,1591,7,7,0,,0,NA,NA,NA,NA,NA
451	2008,1,3,4,617,615,652,650,WN,11,N689SW,95,95,70,2,2,IND,MCI,451,6,19,0,,0,NA,NA,NA,NA,NA
451	2008,1,3,4,1620,1620,1639,1655,WN,810,N648SW,79,95,70,-16,0,IND,MCI,451,3,6,0,,0,NA,NA,NA,NA,NA
text: Unable to write to output stream.
</terminal>

ì´ì œ ì œëŒ€ë¡œ ë‚˜ì˜¤ë„¤ìš”!<br>
ê·¸ëƒ¥ í…ìŠ¤íŠ¸ íŒŒì¼ì€ catë¡œ ì½ì„ ìˆ˜ ìˆì§€ë§Œ ì‹œí€€ìŠ¤ ì••ì¶•ëœ íŒŒì¼ì€ text ëª…ë ¹ì–´ë¡œ ì½ì–´ì•¼ í•œë‹¤ëŠ”ê±¸ ì•Œê²ŒëìŠµë‹ˆë‹¤.<br>
<br>
ë§µ íŒŒì¼ì„ ë§Œë“œëŠ” íŒŒì¼ì„ ë§Œë“¤ì–´ë´…ì‹œë‹¤.<br>
<br>
<color4>MapFileCreator.java</color4>

<terminal>
package Airline;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.SequenceFile.CompressionType;
import org.apache.hadoop.io.compress.GzipCodec;
import org.apache.hadoop.mapred.FileInputFormat;
import org.apache.hadoop.mapred.FileOutputFormat;
import org.apache.hadoop.mapred.JobClient;
import org.apache.hadoop.mapred.JobConf;
import org.apache.hadoop.mapred.MapFileOutputFormat;
import org.apache.hadoop.mapred.SequenceFileInputFormat;
import org.apache.hadoop.mapred.SequenceFileOutputFormat;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;

public class MapFileCreator extends Configured implements Tool {

    public static void main(String[] args) throws Exception {
        int res = ToolRunner.run(new Configuration(), new MapFileCreator(), args);
        System.out.println("###result : " + res);
    }

    public int run(String[] args) throws Exception {    // ë“œë¼ì´ë²„ í´ë˜ìŠ¤
        JobConf conf = new JobConf(MapFileCreator.class);
        conf.setJobName("MapFileCreator");

        FileInputFormat.setInputPaths(conf, new Path(args[0]));
        FileOutputFormat.setOutputPath(conf, new Path(args[1]));

        conf.setInputFormat(SequenceFileInputFormat.class); // ì…ë ¥ íŒŒì¼ì´ ì‹œí€€ìŠ¤ íŒŒì¼
        conf.setOutputFormat(MapFileOutputFormat.class);    // ì¶œë ¥ íŒŒì¼ì€ ë§µ íŒŒì¼
        conf.setOutputKeyClass(IntWritable.class);          // í‚¤ ê°’ì€ distanceì˜€ë˜ ê²ƒì„ ê·¸ëŒ€ë¡œ ìƒì†

        SequenceFileOutputFormat.setCompressOutput(conf, true);                     // ì‹œí€€ìŠ¤ íŒŒì¼ë¡œ ì••ì¶•
        SequenceFileOutputFormat.setOutputCompressorClass(conf, GzipCodec.class);   // GzipCodec ìœ¼ë¡œ ì••ì¶•
        SequenceFileOutputFormat.setOutputCompressionType(conf, CompressionType.BLOCK);

        JobClient.runJob(conf);
        return 0;
    }
}
</terminal>

ë‹¤ í–ˆìœ¼ë©´ ë§µíŒŒì¼ ìƒì„±ì„ í•´ë´…ì‹œë‹¤.<br>

<terminal>
<strong>[hadoop@namenode search]$</strong> javac -cp $HADOOP_HOME/hadoop-core-1.2.1.jar -d . MapFileCreator.java
<strong>[hadoop@namenode search]$</strong> jar -cvf $HADOOP_HOME/MapFileCreator.jar ./Airline/MapFileCreator.class
<strong>[hadoop@namenode search]$</strong> cd $HADOOP_HOME
<strong>[hadoop@namenode hadoop]$</strong> ./bin/hadoop jar MapFileCreator.jar Airline.MapFileCreator 2008_sequencefile/part-* 2008_mapfile
.
.
.
<strong>[hadoop@namenode hadoop]$</strong> ./bin/hadoop fs -ls 2008_mapfile
Found 3 items
-rw-r--r--   3 hadoop supergroup          0 2018-01-28 05:26 /user/hadoop/2008_mapfile/_SUCCESS
drwxr-xr-x   - hadoop supergroup          0 2018-01-28 05:24 /user/hadoop/2008_mapfile/_logs
<strong>drwxr-xr-x</strong>   - hadoop supergroup          0 2018-01-28 05:26 /user/hadoop/2008_mapfile/part-00000
</terminal>

ì œê°€ ì•„ë˜ ì € ë¶€ë¶„ì„ ë‘ê»ê²Œ ê°•ì¡°í•´ë†¨ì£ ?<br>
dë¡œ ì‹œì‘í•˜ë©´ íŒŒì¼ì´ ì•„ë‹ˆë¼ ë””ë ‰í„°ë¦¬ë¼ëŠ” ëœ»ì…ë‹ˆë‹¤.<br>

<terminal>
<strong>[hadoop@namenode hadoop]$</strong> ./bin/hadoop fs -ls 2008_mapfile/part-00000
Found 2 items
-rw-r--r--   3 hadoop supergroup  161302998 2018-01-28 05:25 /user/hadoop/2008_mapfile/part-00000/data
-rw-r--r--   3 hadoop supergroup       7907 2018-01-28 05:25 /user/hadoop/2008_mapfile/part-00000/index
<strong>[hadoop@namenode hadoop]$</strong> ./bin/hadoop fs -text 2008_mapfile/part-00000/data | head -10
18/01/28 05:30:32 INFO util.NativeCodeLoader: Loaded the native-hadoop library
18/01/28 05:30:32 INFO zlib.ZlibFactory: Successfully loaded & initialized native-zlib library
18/01/28 05:30:32 INFO compress.CodecPool: Got brand-new decompressor
18/01/28 05:30:32 INFO compress.CodecPool: Got brand-new decompressor
18/01/28 05:30:32 INFO compress.CodecPool: Got brand-new decompressor
18/01/28 05:30:32 INFO compress.CodecPool: Got brand-new decompressor
11	2008,8,10,7,1315,1220,1415,1320,OH,5572,N819CA,60,60,14,55,55,JFK,LGA,11,8,38,0,,0,55,0,0,0,0
11	2008,5,15,4,2037,1800,2125,1900,OH,4988,N806CA,48,60,31,145,157,JFK,LGA,11,10,7,0,,0,145,0,0,0,0
17	2008,3,8,6,NA,1105,NA,1128,AA,1368,,NA,23,NA,NA,NA,EWR,LGA,17,NA,NA,1,B,0,NA,NA,NA,NA,NA
21	2008,5,9,5,48,100,117,130,AA,588,N061AA,29,30,11,-13,-12,MIA,FLL,21,6,12,0,,0,NA,NA,NA,NA,NA
21	2008,2,8,5,NA,1910,NA,1931,AA,1668,,NA,21,NA,NA,NA,FLL,MIA,21,NA,NA,1,A,0,NA,NA,NA,NA,NA
24	2008,11,27,4,943,940,1014,956,9E,5816,91469E,31,16,9,18,3,IAH,HOU,24,5,17,0,,0,0,0,18,0,0
24	2008,3,12,3,955,931,1021,948,9E,2009,91619E,26,17,10,33,24,IAH,HOU,24,7,9,0,,0,0,0,9,0,24
24	2008,1,2,3,1245,1025,1340,1125,OH,5610,N806CA,55,60,11,135,140,IAD,DCA,24,5,39,0,,0,135,0,0,0,0
28	2008,2,22,5,2046,2050,NA,2156,OO,3698,N298SW,NA,66,NA,NA,-4,SLC,OGD,28,NA,12,0,,1,NA,NA,NA,NA,NA
30	2008,1,6,7,2226,2200,2301,2240,CO,348,N56859,35,40,11,21,26,SJC,SFO,30,7,17,0,,0,0,0,0,0,21
text: Unable to write to output stream.
</terminal>

ë§µíŒŒì¼ë¡œ ì˜ ë§Œë“¤ì–´ì¡ŒìŠµë‹ˆë‹¤.<br>
indexíŒŒì¼ë„ ê¶ê¸ˆí•œë° í•œë²ˆ ë³¼ê¹Œìš”?<br>

<terminal>
<strong>[hadoop@namenode hadoop]$</strong> ./bin/hadoop fs -text 2008_mapfile/part-00000/index | head -10
18/01/28 05:31:20 INFO util.NativeCodeLoader: Loaded the native-hadoop library
18/01/28 05:31:20 INFO zlib.ZlibFactory: Successfully loaded & initialized native-zlib library
18/01/28 05:31:20 INFO compress.CodecPool: Got brand-new decompressor
18/01/28 05:31:20 INFO compress.CodecPool: Got brand-new decompressor
18/01/28 05:31:20 INFO compress.CodecPool: Got brand-new decompressor
18/01/28 05:31:20 INFO compress.CodecPool: Got brand-new decompressor
11	125
31	125
31	125
31	125
31	125
31	125
36	125
41	125
41	125
49	125
text: Unable to write to output stream.
</terminal>

ìŒ.... ì˜ ëª¨ë¥´ê² êµ°ìš”. ë­ ì•„ë¬´íŠ¼ ë‚´ìš©ì´ ìˆêµ¬ë‚˜ í•˜ëŠ”ê±´ ì•Œì•˜ìŠµë‹ˆë‹¤.<br>
ë¶€ë¶„ ì •ë ¬ì˜ ëª©ì ì€ ê²€ìƒ‰ì´ë¼ëŠ” ë§ì”€ì„ ë“œë ¸ì£ ? ì´ì œ ê²€ìƒ‰í•˜ëŠ” ì†ŒìŠ¤ë¥¼ ì§œë³´ê² ìŠµë‹ˆë‹¤.<br>
<br>
<color4>SearchValueList.java</color4>

<terminal>
package Airline;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.MapFile.Reader;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.io.Writable;
import org.apache.hadoop.mapred.MapFileOutputFormat;
import org.apache.hadoop.mapred.Partitioner;
import org.apache.hadoop.mapred.lib.HashPartitioner;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;

public class SearchValueList extends Configured implements Tool {

    public static void main(String[] args) throws Exception {
        int res = ToolRunner.run(new Configuration(), new SearchValueList(), args);
        System.out.println("###result : " + res);
    }

    public int run(String[] args) throws Exception {
        Path path = new Path(args[0]);  // ì…ë ¥ë°›ì€ ì²«ë²ˆ ì§¸ ë°°ì—´(ì…ë ¥ íŒŒì¼ ê²½ë¡œ)
        FileSystem fs = path.getFileSystem(getConf());

        Reader[] readers = MapFileOutputFormat.getReaders(fs, path, getConf()); // ì…ë ¥ íŒŒì¼ ê²½ë¡œ(ë§µ íŒŒì¼)ì„ ë°°ì—´ í˜•ì‹ìœ¼ë¡œ ë°›ì•„ë“¤ì„

        IntWritable key = new IntWritable();
        key.set(Integer.parseInt(args[1])); // ë‘ë²ˆ ì§¸ ë°°ì—´ì„ keyê°’ìœ¼ë¡œ ì‚¼ìŒ

        Text value = new Text();

        Partitioner&lt;IntWritable, Text&gt; partitioner = new HashPartitioner&lt;IntWritable, Text&gt;();  // key, valueë¥¼ ì´ìš©í•´ í•´ì‰¬ íŒŒí‹°ì…”ë„ˆ ìƒì„±
        Reader reader = readers[partitioner.getPartition(key, value, readers.length)];  // ë°°ì—´ í˜•ì‹ìœ¼ë¡œ ë°›ì•„ë“¤ì¸ ë§µ íŒŒì¼ì—ì„œ íŒŒí‹°ì…”ë„ˆë¥¼ ì´ìš©í•´ ê²€ìƒ‰

        Writable entry = reader.get(key, value);
        if (entry == null) {    // ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì„ ë•Œ
            System.out.println("The requested key was not found.");
        }

        IntWritable nextKey = new IntWritable();
        do {
            System.out.println(value.toString());   // ê²€ìƒ‰ ê²°ê³¼ ì¶œë ¥
        } while (reader.next(nextKey, value) && key.equals(nextKey));   // ê²€ìƒ‰ì–´ê°€ ë‹¤ìŒ ë°°ì—´ì˜ keyê°’ê³¼ ê°™ì„ ë•Œ ê¹Œì§€

        return 0;
    }
}
</terminal>

ë‹¤ í–ˆìœ¼ë‹ˆ ì»´íŒŒì¼í•˜ê³  ê²€ìƒ‰ì„ ì‹¤í–‰í•´ë´…ì‹œë‹¤!<br>

<terminal>
<strong>[hadoop@namenode search]$</strong> javac -cp $HADOOP_HOME/hadoop-core-1.2.1.jar -d . SearchValueList.java
<strong>[hadoop@namenode search]$</strong> jar -cvf $HADOOP_HOME/SearchValueList.jar ./Airline/SearchValueList.class
Manifestë¥¼ ì¶”ê°€í•¨
ì¶”ê°€í•˜ëŠ” ì¤‘: Airline/SearchValueList.class(ì…ë ¥ = 2656) (ì¶œë ¥ = 1216)(54%ë¥¼ ê°ì†Œí•¨)
<strong>[hadoop@namenode search]$</strong> cd $HADOOP_HOME
<strong>[hadoop@namenode hadoop]$</strong> ./bin/hadoop fs -rmr 2008_mapfile/_*
Deleted hdfs://namenode:9000/user/hadoop/2008_mapfile/_logs
<strong>[hadoop@namenode hadoop]$</strong> ./bin/hadoop jar SearchValueList.jar Airline.SearchValueList 2008_mapfile 100 | head -10
18/01/28 06:01:58 INFO util.NativeCodeLoader: Loaded the native-hadoop library
18/01/28 06:01:58 INFO zlib.ZlibFactory: Successfully loaded & initialized native-zlib library
18/01/28 06:01:58 INFO compress.CodecPool: Got brand-new decompressor
18/01/28 06:01:58 INFO compress.CodecPool: Got brand-new decompressor
18/01/28 06:01:58 INFO compress.CodecPool: Got brand-new decompressor
18/01/28 06:01:58 INFO compress.CodecPool: Got brand-new decompressor
18/01/28 06:01:58 INFO compress.CodecPool: Got brand-new decompressor
18/01/28 06:01:58 INFO compress.CodecPool: Got brand-new decompressor
18/01/28 06:01:58 INFO compress.CodecPool: Got brand-new decompressor
18/01/28 06:01:58 INFO compress.CodecPool: Got brand-new decompressor
2008,1,29,2,1735,1740,1807,1814,HA,545,N477HA,32,34,22,-7,-5,OGG,HNL,<color4>100</color4>,5,5,0,,0,NA,NA,NA,NA,NA
2008,1,30,3,1740,1740,1815,1814,HA,545,N477HA,35,34,22,1,0,OGG,HNL,<color4>100</color4>,7,6,0,,0,NA,NA,NA,NA,NA
2008,1,31,4,1737,1740,1809,1814,HA,545,N484HA,32,34,22,-5,-3,OGG,HNL,<color4>100</color4>,5,5,0,,0,NA,NA,NA,NA,NA
2008,1,1,2,1629,1635,1705,1712,HA,546,N479HA,36,37,22,-7,-6,HNL,OGG,<color4>100</color4>,5,9,0,,0,NA,NA,NA,NA,NA
2008,1,2,3,1630,1635,1709,1712,HA,546,N481HA,39,37,24,-3,-5,HNL,OGG,<color4>100</color4>,4,11,0,,0,NA,NA,NA,NA,NA
2008,1,3,4,1629,1635,1708,1712,HA,546,N481HA,39,37,24,-4,-6,HNL,OGG,<color4>100</color4>,5,10,0,,0,NA,NA,NA,NA,NA
2008,1,4,5,1632,1635,1710,1712,HA,546,N487HA,38,37,23,-2,-3,HNL,OGG,<color4>100</color4>,7,8,0,,0,NA,NA,NA,NA,NA
2008,1,5,6,1627,1635,1701,1712,HA,546,N484HA,34,37,22,-11,-8,HNL,OGG,<color4>100</color4>,5,7,0,,0,NA,NA,NA,NA,NA
2008,1,6,7,1632,1635,1706,1712,HA,546,N479HA,34,37,20,-6,-3,HNL,OGG,<color4>100</color4>,6,8,0,,0,NA,NA,NA,NA,NA
2008,1,7,1,1630,1635,1705,1712,HA,546,N479HA,35,37,21,-7,-5,HNL,OGG,<color4>100</color4>,5,9,0,,0,NA,NA,NA,NA,NA
</terminal>

distanceê°’ì´ 100ì¸ ê²ƒë§Œ ì¶œë ¥ëœ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.<br>
ì´ëŸ°ì‹ìœ¼ë¡œ íŠ¹ì • ê°’ì— ëŒ€í•œ ê²€ìƒ‰ì´ í•„ìš”í•œ ê²½ìš°ì—ëŠ” ë¶€ë¶„ ì •ë ¬ì„ ì´ìš©í•˜ëŠ”ê²ƒì´ ì¢‹ì„ ê²ƒ ì…ë‹ˆë‹¤.<br>
ì´ë²ˆ ê¸€ì€ ì—¬ê¸°ê¹Œì§€ í•˜ê³ , ë‹¤ìŒ ê¸€ì—ì„œëŠ” ì „ì²´ ì •ë ¬ì„ ë‹¤ë£¨ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.<br>
<br>
<a href="/hadoop/page/2_6.html" onclick="
    event.preventDefault();

    let xhttp = new XMLHttpRequest();

    xhttp.onreadystatechange = () => {
        document.querySelector('main').innerHTML = xhttp.responseText;
    }

    xhttp.open('GET', '/hadoop/page/2_6.html', true);
    xhttp.send();

    document.title = 'Hadoop Guide Part. 2 - Step. 6';
    history.pushState('/hadoop/page/2_6.html' + ' ' + 'Part. 2 - Step. 6', null, '#2_6');

    document.querySelector('side').children[1].classList.add('on');
    document.querySelector('side').children[1].children[5].classList.remove('on');
    document.querySelector('side').children[1].children[6].classList.add('on');
" class="button">ë‹¤ìŒë‹¨ê³„ë¡œ ê°€ê¸°</a>